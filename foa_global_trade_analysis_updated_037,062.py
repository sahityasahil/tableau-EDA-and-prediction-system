# -*- coding: utf-8 -*-
"""FOA_Global_Trade_Analysis_updated_037,062.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D9OmpFDW3M-ANevZQF49RJtukmayGLFb

# **GLOBAL TRADE ANALYSIS - EDA**

---

GIRISH NAIDU RA2111004010037

SAHITYA SAHIL RA2111004010062
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

"""Highlighting the maixmum values of each attribute in the data set"""

df=pd.read_csv('Raw data.csv')
df.shape

df.head()

"""Dropping unwanted column"""

df.drop(columns='quantity_name', axis=1, inplace=True)

df.shape

df.info()

"""Checking for the null values"""

df.isnull().sum()

# filling the null values of 'weight_kg' with the mean of that column
df['weight_kg'].fillna(df['weight_kg'].mean(),inplace=True)

# filling the null values of 'quantity' with the median of that column
df['quantity'].fillna(df['quantity'].median(),inplace=True)

"""Rechecking for any null columns"""

df.isnull().sum()

df.describe()

"""**The 50th percentile i.e. the Median value < Mean value OR Mean > Median --> SKEWED DATA**

Removing some unwanted columns with 'flow' = Re-Import
"""

# Creating a mask to remove specific columns
mask = df['flow'] == 'Re-Import'
df1 = df[~mask]

#Rechecking if the rows are removed or not
df1.shape

#Again doing the same with 'flow'= Re-Export
mask = df1['flow'] == 'Re-Export'
df2 = df1[~mask]

df2.shape

"""Checking the distribution of target variable"""

df2['flow'].value_counts()

# encoding the target column
label_encode = LabelEncoder()

labels = label_encode.fit_transform(df2['flow'])
df2['target'] = labels
df2.drop(columns='flow', axis=1, inplace=True)

#Flow column removed
df2.head()

df2['target'].value_counts()

"""For the above:

Export -> 0

Import -> 1

The data above is almost balanced.
"""

# Grouping the data based on target

df2.groupby('target').mean()

"""# **DATA VISUALIZATION**"""

# countplot for the target column for checking the distribution of the target
sns.countplot(x='target', data=df2)

"""Distribution plot for all columns"""

# this is how we can get all the column names of the dataframe
for column in df2:
  print(column)

# scatterplot for 1st feature(x) alongwith second feature(y)
plt.scatter(x=df2.trade_usd, y=df2.weight_kg)

correlation_matrix = df2.corr()

"""**Heat map to visualize the correlation matrix:**"""

plt.figure(figsize=(10,10))
sns.heatmap(correlation_matrix, cbar=True, fmt='.1f', annot=True, cmap='Blues')
plt.savefig('Correlation Heat Map')

plt.plot(df2['trade_usd'],df2['year'], label='Line Plot')
plt.xlabel('Trade in USD')
plt.ylabel('Year')
plt.title('Trade Line Plot')
plt.legend()
plt.show()

"""# **PREDICTION MODEL**"""

# dropping the string type columns
df2 = df2.drop(columns = 'country_or_area', axis=1)

# dropping the string type columns
df2 = df2.drop(columns = 'commodity', axis=1)

# dropping the string type columns
df2 = df2.drop(columns = 'category', axis=1)

# dropping the string type columns
df2 = df2.drop(columns = 'year', axis=1)

# dropping the string type columns
df2 = df2.drop(columns = 'comm_code', axis=1)

df2.head()

# seperating the data and labels
X = df2.drop(columns = 'target', axis=1)
Y = df2['target']

print(X)

print(Y)

"""Data Standardization"""

scaler = StandardScaler()

scaler.fit(X)

standardized_data = scaler.transform(X)

print(standardized_data)

X = standardized_data
Y = df2['target']

print(X)
print(Y)

"""Train-Test Split"""

X_train, X_test, Y_train, Y_test =  train_test_split(X,Y,test_size = 0.5, stratify=Y, random_state=2)

print(X.shape, X_train.shape, X_test.shape)

"""Training the model"""

classifier = svm.SVC(kernel='linear')

#training the support vector Machine Classifier
classifier.fit(X_train, Y_train)

"""***Model Evaluation***

Accuracy score
"""

# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score(X_train_prediction, Y_train)

print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_prediction, Y_test)

print('Accuracy score of the test data : ', test_data_accuracy)

"""Making a predictive system"""

input_data = (5221,1000.0,2.0)

# changing the input data into numpy array
input_data_as_numpy_array = np.asarray(input_data)

#reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

#standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('Product Exported')
else:
  print('Product Imported')

"""Saving the trained model"""

import pickle

filename = 'trained_model.sav'
pickle.dump(classifier, open(filename, 'wb'))

# loading the saved model
loading_model = pickle.load(open('trained_model.sav', 'rb'))

input_data = (5221,1000.0,2.0)

# changing the input data into numpy array
input_data_as_numpy_array = np.asarray(input_data)

#reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

#standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('Product Exported')
else:
  print('Product Imported')